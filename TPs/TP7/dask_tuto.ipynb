{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09efc1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask as da\n",
    "from dask.distributed import Client\n",
    "from dask import delayed\n",
    "import pandas as pd\n",
    "import time\n",
    "from time import sleep\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64835da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inc(x):\n",
    "    sleep(1)\n",
    "    return x + 1\n",
    "\n",
    "def add(x, y):\n",
    "    sleep(1)\n",
    "    return x + y\n",
    "\n",
    "\n",
    "def seq_eval():\n",
    "    x = inc(1)\n",
    "    y = inc(2)\n",
    "    z = add(x, y)\n",
    "\n",
    "def delayed_eval():\n",
    "    x = delayed(inc)(1)\n",
    "    y = delayed(inc)(2)\n",
    "    z = delayed(add)(x, y)\n",
    "    return z\n",
    "\n",
    "def test_delayed():\n",
    "    client = Client(n_workers=4)\n",
    "\n",
    "    print(\"SEQ EVAL:\")\n",
    "    t0 = time.process_time()\n",
    "    seq_eval()\n",
    "    t1 = time.process_time() - t0\n",
    "    print(\"TIME SEQ :\", t1)\n",
    "\n",
    "    print(\"DELAYED EVAL:\")\n",
    "    t0 = time.process_time()\n",
    "    z = delayed_eval()\n",
    "    t1 = time.process_time() - t0\n",
    "    print(\"TIME LAZY :\", t1)\n",
    "    #exit(0)\n",
    "\n",
    "    t0 = time.process_time()\n",
    "    z.compute()\n",
    "    t1 = time.process_time() - t0\n",
    "    print(\"TIME COMPUTE :\", t1)\n",
    "    z.visualize()\n",
    "\n",
    "    client.close()\n",
    "\n",
    "def seq_out_core_eval(dset):\n",
    "    # Compute sum of large array, one million numbers at a time\n",
    "    sums = []\n",
    "    for i in range(0, 1000000000, 1000000):\n",
    "        chunk = dset[i: i + 1000000]  # pull out numpy array\n",
    "        sums.append(chunk.sum())\n",
    "    total = sum(sums)\n",
    "    print(\"TOTAL : \",total)\n",
    "\n",
    "def dask_out_core_eval(dset):\n",
    "    import dask.array as da\n",
    "    x = da.from_array(dset, chunks=(1000000,))\n",
    "    result = x.sum()\n",
    "    return result\n",
    "\n",
    "def test_out_of_memory(data_dir):\n",
    "    import h5py\n",
    "    import os\n",
    "\n",
    "    client = Client(n_workers=4, processes=False)\n",
    "    f = h5py.File(os.path.join(data_dir,'data', 'random.hdf5'), mode='r')\n",
    "    dset = f['/x']\n",
    "\n",
    "    print(\"SEQ SUM EVAL\")\n",
    "    t0 = time.process_time()\n",
    "    seq_out_core_eval(dset)\n",
    "    t1 = time.process_time() - t0\n",
    "    print(\"TIME SEQ :\", t1)\n",
    "\n",
    "    print(\"DASK SUM LAZY EVAL\")\n",
    "    t0 = time.process_time()\n",
    "    result = dask_out_core_eval(dset)\n",
    "    t1 = time.process_time() - t0\n",
    "    print(\"TIME DASK :\", t1)\n",
    "\n",
    "    print(\"DASK SUM COMPUTE\")\n",
    "    t0 = time.process_time()\n",
    "    total = result.compute()\n",
    "    t1 = time.process_time() - t0\n",
    "    print(\"TIME DASK2 :\", t1)\n",
    "    print(\"TOTAL : \",total)\n",
    "    client.close()\n",
    "\n",
    "\n",
    "\n",
    "def test_dask_array(data_dir):\n",
    "    import dask.array as da\n",
    "    import h5py\n",
    "    from glob import glob\n",
    "    import os\n",
    "\n",
    "    client = Client(n_workers=4, processes=False)\n",
    "\n",
    "    filenames = sorted(glob(os.path.join(data_dir,'data', 'weather-big', '*.hdf5')))\n",
    "    dsets = [h5py.File(filename, mode='r')['/t2m'] for filename in filenames]\n",
    "    print(dsets[0])\n",
    "\n",
    "    #fig = plt.figure(figsize=(16, 8))\n",
    "    #plt.imshow(dsets[0][::4, ::4], cmap='RdBu_r')\n",
    "    #plt.show()\n",
    "\n",
    "    arrays = [da.from_array(dset, chunks=(500, 500)) for dset in dsets]\n",
    "    print(arrays)\n",
    "\n",
    "    x = da.stack(arrays, axis=0)\n",
    "    print('X : \\n',x)\n",
    "\n",
    "\n",
    "    print('COMPUTE MEAN X')\n",
    "    result = x[0] - x.mean(axis=0)\n",
    "\n",
    "    print('PLOT MEAN X')\n",
    "    fig = plt.figure(figsize=(16, 8))\n",
    "    plt.imshow(result, cmap='RdBu_r')\n",
    "    plt.show()\n",
    "\n",
    "    client.close()\n",
    "\n",
    "def test_dataframe(data_dir):\n",
    "    from dask.distributed import Client\n",
    "    import dask.dataframe as dd\n",
    "    from dask import compute\n",
    "    import glob\n",
    "\n",
    "    #from prep import accounts_csvs\n",
    "    #accounts_csvs()\n",
    "\n",
    "    client = Client(n_workers=4)\n",
    "\n",
    "    filename = os.path.join(data_dir,'data', 'accounts.*.csv')\n",
    "    print(filename)\n",
    "\n",
    "    df = dd.read_csv(filename)\n",
    "    df.head()\n",
    "    # load and count number of rows\n",
    "    print('LEN',len(df))\n",
    "\n",
    "    df = dd.read_csv(os.path.join(data_dir,'data', 'nycflights', '*.csv'),\n",
    "                     parse_dates={'Date': [0, 1, 2]})\n",
    "    print(df)\n",
    "    df.head()\n",
    "    df = dd.read_csv(os.path.join(data_dir,'data', 'nycflights', '*.csv'),\n",
    "                     parse_dates={'Date': [0, 1, 2]},\n",
    "                     dtype={'TailNum': str,\n",
    "                            'CRSElapsedTime': float,\n",
    "                            'Cancelled': bool})\n",
    "    df.tail()\n",
    "\n",
    "\n",
    "    filenames = glob.glob(os.path.join(data_dir,'data', 'nycflights', '*.csv'))\n",
    "    t0 = time.process_time()\n",
    "    maxes = []\n",
    "    for fn in filenames:\n",
    "        df = pd.read_csv(fn)\n",
    "        maxes.append(df.DepDelay.max())\n",
    "    final_max = max(maxes)\n",
    "    t1 = time.process_time() - t0\n",
    "    print(\"MAX:\",final_max)\n",
    "    print(\"TIME COMPUTE MAX :\", t1)\n",
    "\n",
    "    t0 = time.process_time()\n",
    "    df = dd.read_csv(os.path.join(data_dir, 'data', 'nycflights', '*.csv'),\n",
    "                     parse_dates={'Date': [0, 1, 2]},\n",
    "                     dtype={'TailNum': str,'CRSElapsedTime': float,'Cancelled': bool})\n",
    "    print(\"DASK MAX:\",df.DepDelay.max().compute())\n",
    "    t1 = time.process_time() - t0\n",
    "    print(\"TIME DASK COMPUTE MAX :\", t1)\n",
    "    df.DepDelay.max().visualize()\n",
    "\n",
    "    help(df.CRSDepTime.map_partitions)\n",
    "    client.close()\n",
    "\n",
    "def test_map_partition(data_dir):\n",
    "    from dask.distributed import Client\n",
    "    import dask.dataframe as dd\n",
    "    from dask import compute\n",
    "    import glob\n",
    "\n",
    "\n",
    "    client = Client(n_workers=4)\n",
    "\n",
    "    df = dd.read_csv(os.path.join(data_dir,'data', 'nycflights', '*.csv'),\n",
    "                     parse_dates={'Date': [0, 1, 2]},\n",
    "                     dtype={'TailNum': str,\n",
    "                            'CRSElapsedTime': float,\n",
    "                            'Cancelled': bool})\n",
    "    df.tail()\n",
    "\n",
    "    help(df.CRSDepTime.map_partitions)\n",
    "    client.close()\n",
    "\n",
    "def load_array(ip,nx,ny):\n",
    "    import numpy as np\n",
    "    a = np.zeros((nx,ny),dtype=int)\n",
    "    a[:,:] = ip\n",
    "    return a\n",
    "\n",
    "def test5():\n",
    "    import numpy as np\n",
    "    from dask.distributed import Client\n",
    "    client = Client(n_workers=4)\n",
    "    nx=10\n",
    "    ny=5\n",
    "    partitions = [0,1,2,3]\n",
    "    arrays = [ delayed(load_array)(ip,nx,ny) for ip in partitions]\n",
    "    results = da.compute(*arrays)\n",
    "    for r in results:\n",
    "        print('R:',r.shape)\n",
    "    new_array = np.concatenate(results,axis=0)\n",
    "    print('NEW SHAPE',new_array.shape)\n",
    "    print(\"NEW ARRAY\",new_array)\n",
    "\n",
    "    client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea19f81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"-t\", \"--test\", help=\"no du test\", default=\"0\")\n",
    "args = parser.parse_args()\n",
    "data_dir = '/work/gratienj/ParallelProgrammingCourse/GIT/BigDataHadoopSparkDaskCourse/TPs/data/dask'\n",
    "test_id = int(args.test)\n",
    "if test_id == 0:\n",
    "    test_delayed()\n",
    "if test_id == 1:\n",
    "    test_out_of_memory(data_dir)\n",
    "if test_id == 2:\n",
    "    test_dask_array(data_dir)\n",
    "if test_id == 3:\n",
    "    test_dataframe(data_dir)\n",
    "if test_id == 4:\n",
    "    test_map_partition(data_dir)\n",
    "if test_id == 5:\n",
    "    test5()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
