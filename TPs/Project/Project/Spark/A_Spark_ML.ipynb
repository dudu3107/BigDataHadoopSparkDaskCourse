{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79ed5585",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark import SparkFiles\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aef442cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (1.4.0)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.9/dist-packages (from pandas) (1.22.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 22.0.3 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "138a468e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/02/19 00:28:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/02/19 00:28:19 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"----------------------------------------------------------------------------\n",
    "CREATE SPARK CONTEXT\n",
    "CREATE SQL CONTEXT\n",
    "----------------------------------------------------------------------------\"\"\"\n",
    "sc = SparkContext()\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "400509bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sepal_length: double (nullable = true)\n",
      " |-- sepal_width: double (nullable = true)\n",
      " |-- petal_length: double (nullable = true)\n",
      " |-- petal_width: double (nullable = true)\n",
      " |-- variety: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sepal_length: double (nullable = true)\n",
      " |-- sepal_width: double (nullable = true)\n",
      " |-- petal_length: double (nullable = true)\n",
      " |-- petal_width: double (nullable = true)\n",
      " |-- variety: string (nullable = true)\n",
      " |-- ind_variety: double (nullable = false)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'--------------------------------------------------------------------------\\nPerform Data Analytics\\n-------------------------------------------------------------------------'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"----------------------------------------------------------------------------\n",
    "LOAD IRIS DATA\n",
    "----------------------------------------------------------------------------\"\"\"\n",
    "data_dir=\"./data/\"\n",
    "file = os.path.join(data_dir,\"iris.csv\")\n",
    "panda_df = pd.read_csv(file)\n",
    "\n",
    "iris_df=sqlContext.createDataFrame(panda_df)\n",
    "iris_df.printSchema()\n",
    "\n",
    "#Add a numeric indexer for the label/target column\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "stringIndexer = StringIndexer(inputCol=\"variety\", outputCol=\"ind_variety\")\n",
    "si_model = stringIndexer.fit(iris_df)\n",
    "irisNormDf = si_model.transform(iris_df)\n",
    "irisNormDf.printSchema()\n",
    "irisNormDf.select(\"variety\",\"ind_variety\").distinct().collect()\n",
    "#irisNormDf.cache()\n",
    "\n",
    "\"\"\"--------------------------------------------------------------------------\n",
    "Perform Data Analytics\n",
    "-------------------------------------------------------------------------\"\"\"\n",
    "\n",
    "#See standard parameters\n",
    "#irisNormDf.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4638f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+-----------------+\n",
      "|species|label|         features|\n",
      "+-------+-----+-----------------+\n",
      "| Setosa|  0.0|[5.1,3.5,1.4,0.2]|\n",
      "| Setosa|  0.0|[4.9,3.0,1.4,0.2]|\n",
      "| Setosa|  0.0|[4.7,3.2,1.3,0.2]|\n",
      "| Setosa|  0.0|[4.6,3.1,1.5,0.2]|\n",
      "| Setosa|  0.0|[5.0,3.6,1.4,0.2]|\n",
      "| Setosa|  0.0|[5.4,3.9,1.7,0.4]|\n",
      "| Setosa|  0.0|[4.6,3.4,1.4,0.3]|\n",
      "| Setosa|  0.0|[5.0,3.4,1.5,0.2]|\n",
      "| Setosa|  0.0|[4.4,2.9,1.4,0.2]|\n",
      "| Setosa|  0.0|[4.9,3.1,1.5,0.1]|\n",
      "| Setosa|  0.0|[5.4,3.7,1.5,0.2]|\n",
      "| Setosa|  0.0|[4.8,3.4,1.6,0.2]|\n",
      "| Setosa|  0.0|[4.8,3.0,1.4,0.1]|\n",
      "| Setosa|  0.0|[4.3,3.0,1.1,0.1]|\n",
      "| Setosa|  0.0|[5.8,4.0,1.2,0.2]|\n",
      "| Setosa|  0.0|[5.7,4.4,1.5,0.4]|\n",
      "| Setosa|  0.0|[5.4,3.9,1.3,0.4]|\n",
      "| Setosa|  0.0|[5.1,3.5,1.4,0.3]|\n",
      "| Setosa|  0.0|[5.7,3.8,1.7,0.3]|\n",
      "| Setosa|  0.0|[5.1,3.8,1.5,0.3]|\n",
      "| Setosa|  0.0|[5.4,3.4,1.7,0.2]|\n",
      "| Setosa|  0.0|[5.1,3.7,1.5,0.4]|\n",
      "| Setosa|  0.0|[4.6,3.6,1.0,0.2]|\n",
      "| Setosa|  0.0|[5.1,3.3,1.7,0.5]|\n",
      "| Setosa|  0.0|[4.8,3.4,1.9,0.2]|\n",
      "| Setosa|  0.0|[5.0,3.0,1.6,0.2]|\n",
      "| Setosa|  0.0|[5.0,3.4,1.6,0.4]|\n",
      "| Setosa|  0.0|[5.2,3.5,1.5,0.2]|\n",
      "| Setosa|  0.0|[5.2,3.4,1.4,0.2]|\n",
      "| Setosa|  0.0|[4.7,3.2,1.6,0.2]|\n",
      "| Setosa|  0.0|[4.8,3.1,1.6,0.2]|\n",
      "| Setosa|  0.0|[5.4,3.4,1.5,0.4]|\n",
      "| Setosa|  0.0|[5.2,4.1,1.5,0.1]|\n",
      "| Setosa|  0.0|[5.5,4.2,1.4,0.2]|\n",
      "| Setosa|  0.0|[4.9,3.1,1.5,0.2]|\n",
      "| Setosa|  0.0|[5.0,3.2,1.2,0.2]|\n",
      "| Setosa|  0.0|[5.5,3.5,1.3,0.2]|\n",
      "| Setosa|  0.0|[4.9,3.6,1.4,0.1]|\n",
      "| Setosa|  0.0|[4.4,3.0,1.3,0.2]|\n",
      "| Setosa|  0.0|[5.1,3.4,1.5,0.2]|\n",
      "| Setosa|  0.0|[5.0,3.5,1.3,0.3]|\n",
      "| Setosa|  0.0|[4.5,2.3,1.3,0.3]|\n",
      "| Setosa|  0.0|[4.4,3.2,1.3,0.2]|\n",
      "| Setosa|  0.0|[5.0,3.5,1.6,0.6]|\n",
      "| Setosa|  0.0|[5.1,3.8,1.9,0.4]|\n",
      "| Setosa|  0.0|[4.8,3.0,1.4,0.3]|\n",
      "| Setosa|  0.0|[5.1,3.8,1.6,0.2]|\n",
      "| Setosa|  0.0|[4.6,3.2,1.4,0.2]|\n",
      "| Setosa|  0.0|[5.3,3.7,1.5,0.2]|\n",
      "| Setosa|  0.0|[5.0,3.3,1.4,0.2]|\n",
      "+-------+-----+-----------------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[species: string, label: double, features: vector]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"--------------------------------------------------------------------------\n",
    "Prepare data for ML\n",
    "-------------------------------------------------------------------------\"\"\"\n",
    "\n",
    "#Transform to a Data Frame for input to Machine Learing\n",
    "#Drop columns that are not required (low correlation)\n",
    "\n",
    "from pyspark.ml.linalg import Vectors\n",
    "def transformToLabeledPoint(row) :\n",
    "    lp = ( row[\"variety\"], row[\"ind_variety\"], \\\n",
    "                Vectors.dense([row[\"sepal_length\"],\\\n",
    "                        row[\"sepal_width\"], \\\n",
    "                        row[\"petal_length\"], \\\n",
    "                        row[\"petal_width\"]]))\n",
    "    return lp\n",
    "\n",
    "irisLp = irisNormDf.rdd.map(transformToLabeledPoint)\n",
    "irisLpDf = sqlContext.createDataFrame(irisLp,[\"species\",\"label\", \"features\"])\n",
    "irisLpDf.select(\"species\",\"label\",\"features\").show(50)\n",
    "irisLpDf.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5177f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|  1.0|       1.0|    6|\n",
      "|  2.0|       2.0|    3|\n",
      "|  2.0|       1.0|    1|\n",
      "|  1.0|       2.0|    1|\n",
      "|  0.0|       0.0|    4|\n",
      "+-----+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"--------------------------------------------------------------------------\n",
    "Perform Machine Learning\n",
    "-------------------------------------------------------------------------\"\"\"\n",
    "\n",
    "#Split into training and testing data\n",
    "(trainingData, testData) = irisLpDf.randomSplit([0.9, 0.1])\n",
    "trainingData.count()\n",
    "testData.count()\n",
    "testData.collect()\n",
    "\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "\n",
    "#Create the model\n",
    "dtClassifer = DecisionTreeClassifier(maxDepth=4, labelCol=\"label\",\\\n",
    "                featuresCol=\"features\")\n",
    "from pyspark.ml import Pipeline\n",
    "dtpipeline = Pipeline(stages=[dtClassifer])\n",
    "dtModel = dtpipeline.fit(trainingData)\n",
    "#dtModel = dtClassifer.fit(trainingData)\n",
    "\n",
    "#print(dtModel.numNodes)\n",
    "#print(dtModel.depth)\n",
    "\n",
    "#Predict on the test data\n",
    "predictions = dtModel.transform(testData)\n",
    "predictions.select(\"prediction\",\"species\",\"label\").collect()\n",
    "\n",
    "#Evaluate accuracy\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", \\\n",
    "                    labelCol=\"label\",metricName=\"accuracy\")\n",
    "evaluator.evaluate(predictions)    \n",
    "\n",
    "#Draw a confusion matrix\n",
    "predictions.groupBy(\"label\",\"prediction\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66f04d63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8666666666666667"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(predictions)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36dccf2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|  1.0|       1.0|    6|\n",
      "|  2.0|       2.0|    4|\n",
      "|  0.0|       0.0|    6|\n",
      "+-----+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"--------------------------------------------------------------------------\n",
    "Perform Machine Learning RF\n",
    "-------------------------------------------------------------------------\"\"\"\n",
    "\n",
    "#Split into training and testing data\n",
    "(trainingData, testData) = irisLpDf.randomSplit([0.9, 0.1])\n",
    "trainingData.count()\n",
    "testData.count()\n",
    "testData.collect()\n",
    "\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "#Create the model\n",
    "rfClassifer = RandomForestClassifier(maxDepth=4, labelCol=\"label\",\\\n",
    "                featuresCol=\"features\")\n",
    "from pyspark.ml import Pipeline\n",
    "rfpipeline = Pipeline(stages=[rfClassifer])\n",
    "rfModel = rfpipeline.fit(trainingData)\n",
    "#rfModel = rfClassifer.fit(trainingData)\n",
    "\n",
    "#print(dtModel.numNodes)\n",
    "#print(dtModel.depth)\n",
    "\n",
    "#Predict on the test data\n",
    "rfpredictions = rfModel.transform(testData)\n",
    "rfpredictions.select(\"prediction\",\"species\",\"label\").collect()\n",
    "\n",
    "#Evaluate accuracy\n",
    "rfevaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", \\\n",
    "                    labelCol=\"label\",metricName=\"accuracy\")\n",
    "rfevaluator.evaluate(rfpredictions)    \n",
    "\n",
    "#Draw a confusion matrix\n",
    "rfpredictions.groupBy(\"label\",\"prediction\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b59a7665",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfevaluator.evaluate(rfpredictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59a77a09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/02/19 00:28:50 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "22/02/19 00:28:50 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|  1.0|       1.0|    9|\n",
      "|  0.0|       0.0|    6|\n",
      "+-----+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"--------------------------------------------------------------------------\n",
    "Perform Machine Learning GB\n",
    "-------------------------------------------------------------------------\"\"\"\n",
    "\n",
    "from pyspark.sql.functions import when, lit\n",
    "irisLpDfBis = irisLpDf\n",
    "irisLpDfBis = irisLpDfBis.withColumn('label', when(irisLpDfBis.label==2.0, \n",
    "lit(1.0)).otherwise(irisLpDfBis.label))\n",
    "#irisLpDfBis.select(\"species\",\"label\",\"features\").show(500)\n",
    "#irisLpDfBisB = irisLpDfBis.withColumn('label', when(irisLpDfBis.label==2.0, \n",
    "#lit(0.0)).otherwise(irisLpDfBis.label))\n",
    "#irisLpDfBisC = irisLpDfBis.withColumn('label', when(irisLpDfBis.label==0.0, \n",
    "#lit(1.0)).otherwise(irisLpDfBis.label))\n",
    "\n",
    "#Split into training and testing data\n",
    "(trainingDataBis, testDataBis) = irisLpDfBis.randomSplit([0.9, 0.1])\n",
    "trainingDataBis.count()\n",
    "testDataBis.count()\n",
    "testDataBis.collect()\n",
    "\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "#Create the model\n",
    "gbtClassifer = GBTClassifier(maxDepth=4, labelCol=\"label\",\\\n",
    "                featuresCol=\"features\")\n",
    "from pyspark.ml import Pipeline\n",
    "gbtpipeline = Pipeline(stages=[gbtClassifer])\n",
    "gbtModel = gbtpipeline.fit(trainingDataBis)\n",
    "#gbtModel = gbtClassifer.fit(trainingData)\n",
    "\n",
    "#print(dtModel.numNodes)\n",
    "#print(dtModel.depth)\n",
    "\n",
    "#Predict on the test data\n",
    "gbtpredictions = gbtModel.transform(testDataBis)\n",
    "gbtpredictions.select(\"prediction\",\"species\",\"label\").collect()\n",
    "\n",
    "#Evaluate accuracy\n",
    "gbtevaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", \\\n",
    "                    labelCol=\"label\",metricName=\"accuracy\")\n",
    "gbtevaluator.evaluate(gbtpredictions)    \n",
    "\n",
    "#Draw a confusion matrix\n",
    "gbtpredictions.groupBy(\"label\",\"prediction\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad903ad8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbtevaluator.evaluate(gbtpredictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d78c5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.classification import OneVsRest\n",
    "\n",
    "#Create the model\n",
    "gbtClassiferBis = GBTClassifier(maxDepth=4, labelCol=\"label\",\\\n",
    "                featuresCol=\"features\")\n",
    "ovr = OneVsRest(classifier=gbtClassiferBis)\n",
    "ovr.setPredictionCol(\"prediction\")\n",
    "gbtModelBis = ovr.fit(irisLpDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6629abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-----+\n",
      "|label|prediction|count|\n",
      "+-----+----------+-----+\n",
      "|  1.0|       1.0|    6|\n",
      "|  2.0|       2.0|    4|\n",
      "|  0.0|       0.0|    6|\n",
      "+-----+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Predict on the test data\n",
    "gbtpredictionsBis = gbtModelBis.transform(testData)\n",
    "gbtpredictionsBis.select(\"prediction\",\"species\",\"label\").collect()\n",
    "\n",
    "#Evaluate accuracy\n",
    "#gbtevaluatorBis = MulticlassClassificationEvaluator(predictionCol=\"prediction\", \\\n",
    "#                    labelCol=\"label\",metricName=\"accuracy\")\n",
    "gbtevaluator.evaluate(gbtpredictionsBis)\n",
    "\n",
    "#Draw a confusion matrix\n",
    "gbtpredictionsBis.groupBy(\"label\",\"prediction\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e28dc4e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbtevaluator.evaluate(gbtpredictionsBis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "92026224",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBT accuracy:  1.0\n",
      "RF accuracy:   1.0\n",
      "DT accuracy:   0.8666666666666667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/02/19 03:27:49 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 7843331 ms exceeds timeout 120000 ms\n",
      "22/02/19 03:27:49 WARN SparkContext: Killing executors is not supported by current scheduler.\n"
     ]
    }
   ],
   "source": [
    "print(\"GBT accuracy: \", gbtevaluator.evaluate(gbtpredictionsBis))\n",
    "print(\"RF accuracy:  \", rfevaluator.evaluate(rfpredictions))\n",
    "print(\"DT accuracy:  \", evaluator.evaluate(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd1c0f5",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages=[gbtClassifer])\n",
    "  \n",
    "model = pipeline.fit(trainingData)\n",
    "gbt2prediction = model.transform(trainingData)\n",
    "gbt2prediction.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ba792f",
   "metadata": {},
   "source": [
    "gbt2evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\", \\\n",
    "                    labelCol=\"label\",metricName=\"accuracy\")\n",
    "gbt2evaluator.evaluate(gbt2predictions) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848134a0",
   "metadata": {},
   "source": [
    "from pyspark.sql.functions import when, lit\n",
    "irisLpDfBis = irisLpDf\n",
    "irisLpDfBis = irisLpDfBis.withColumn('label', when(irisLpDfBis.label==2.0, \n",
    "lit('1.0')).otherwise(irisLpDfBis.label))\n",
    "irisLpDfBis.select(\"species\",\"label\",\"features\").show(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04df895a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
